{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "038579d4",
   "metadata": {},
   "source": [
    "#### Project guidelines \n",
    "\n",
    "https://github.com/suneman/socialdata2021/wiki/Final-Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7b9990",
   "metadata": {},
   "source": [
    "# Motivation.\n",
    "\n",
    "\n",
    "### What is your dataset?\n",
    "* Hourly traffic data for some of Copenhagen busiest roads. The data have been collected between 2005 and 2014 and made available on the [Copenhagen Open Traffic Data page](https://www.opendata.dk/city-of-copenhagen/faste-trafiktaellinger#resource-faste_trafikt%C3%A6llinger_2008.xlsx)\n",
    "*  We have merged each year of data into one dataset that we stored [here](https://github.com/LarsBryld/socialdata/blob/main/cph_traffic_2005-2009_original.csv) and [here](https://github.com/LarsBryld/socialdata/blob/main/cph_traffic_2010-2014_original.csv) (unfortunately GitHub doesn't allow to upload files bigger than 25MB so we had to split our dataset in 2 separate files)\n",
    "\n",
    "* The weather data have been downloaded from the [Danish Meteorological Institute archive](https://www.dmi.dk/vejrarkiv/)\n",
    "\n",
    "* Copenhagen GeoJson polygons (districts maps) have been downloaded from [here](https://giedriusk.carto.com/tables/copenhagen_districts/public)\n",
    "\n",
    "\n",
    "### Why did you choose this/these particular dataset(s)?\n",
    "We were interested in:\n",
    "   - describing CPH traffic flows over time and space \n",
    "   - identify patterns in the data that allow for classification of traffic volumes by roads\n",
    "   - combine Copenhagen traffic data with weather data to try predict traffic volumes over time and space\n",
    "\n",
    "### What was your goal for the end user's experience?\n",
    "Building a tool that allows for an easy visualizion of Copenhagen traffic volumes/flows across time/space \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Basic stats. Let's understand the dataset better\n",
    "\n",
    "### Traffic dataset stats\n",
    "\n",
    "The dataset has **183k rows**, with **30 columns** and after cleaning, preprocessing and transformation of the data it has increased to 1.4 million rows. \n",
    "The original columns contain the following information:\n",
    "   - Vej-Id, these markers end with values \"T\", \"+\" or \"-\":\n",
    "        - \"T\" are Total vehicles detected on each hour/road/detection point;\n",
    "        - \"+\" are the vehicles moving in the direction of increasing house numbers: \"+\" means that the house numbers go up (1,2,3....). We assumed that the roads numbering starts from the city center and increases with the distance. So intuitively these vehicles should be the ones leaving the city. This is often true, although not always, as confirmed by the CPH traffic data owners, but we will show in our visualizations that it holds for most roads;\n",
    "        - \"-\" should then be the vehicles entering the city (vehicles that physically move \"against\" the house numbering);\n",
    "   - Vejnavn contains the Road Names;\n",
    "   - UTM geographical coordinates of the traffic detection points;\n",
    "   - Date of detection;\n",
    "   - Hour of detection: basically there are 24 traffic columns for each date row;\n",
    "  \n",
    "### Our choices in data cleaning, preprocessing and data transformation\n",
    "  \n",
    "The original data have been transformed in the following ways:\n",
    "   - Vej-Id data have been manipulated to isolate only the last element: \"T\", \"+\", \"-\";\n",
    "   - UTM coordinates have been transformed into Latitude and Logitude coordinates;\n",
    "   - hourly column data have been convereted into rows. This has increased the number of rows by a factor of 24, to around 4.4m rows;\n",
    "   - Vej-Id markers have been used to move \"Leaving\" and \"Entering\" traffic data from columns to rows. This has allowed us to create 3 new features: Leaving vehichles, Entering vehichles and \"Net Traffic Flow\" data, that are very useful in showing hourly traffic patterns and for our ML classiffication tool. This transformation has of course reduced the number of rows to 1/3 to around 1.45m rows;\n",
    "   - we have randomized Latitude and Longitude, and added these data in 2 new columns (\"Lat_rand\", \"Lon_rand\"). this was necessary to facilitate spacial data visualizations;  \n",
    "   - Finally we have created new features to visualize the data by different timeframes: daily, weekly, yearly, etc.\n",
    "   - As part of our data preprocessing we have also deleted all the empty or otherwise irrelavant columns of data\n",
    "\n",
    "\n",
    "### Do we need to add info about Maps/Weather data here???\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15190d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import utm\n",
    "import folium\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5ebeea",
   "metadata": {},
   "source": [
    "Downloading data and removing/adding features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c77d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading faste-trafiktaellinger-2008_clean (to be changed if we get data directly form website)\n",
    "#df = pd.read_csv(\"C:/Users/User/Dropbox/DTU/02806 Social data analysis and visualization/cph_traffic_2005-2014_original.csv\",\n",
    "#                 parse_dates = ['Dato'],encoding='ISO-8859-1')\n",
    "\n",
    "df1 = pd.read_csv(\"https://raw.githubusercontent.com/LarsBryld/socialdata/main/cph_traffic_2005-2009_original.csv\",\n",
    "                 parse_dates = ['Dato'],encoding='ISO-8859-1')\n",
    "df2 = pd.read_csv(\"https://raw.githubusercontent.com/LarsBryld/socialdata/main/cph_traffic_2010-2014_original.csv\",\n",
    "                 parse_dates = ['Dato'],encoding='ISO-8859-1')\n",
    "\n",
    "df = pd.concat((df1,df2))\n",
    "\n",
    "# cleaning Vej-Id for more clear traffic directions\n",
    "df['Vej-Id'] = df['Vej-Id'].str.split(n=4).str[-1]\n",
    "\n",
    "#change the Hours headers\n",
    "for i in range(7,31):\n",
    "    df = df.rename(columns={df.columns[i]: df.columns[i].split('.')[1].split('-')[0]})\n",
    "    df[df.columns[i]] = df[df.columns[i]].str.replace(',', '').fillna(0).astype('float')\n",
    "\n",
    "### converting UTM coordinates into Latitute/Longitude using the \"utm\" library: https://pypi.org/project/utm/\n",
    "# first we create a function that applies the utm api to 2 Series of data\n",
    "def uf(x):\n",
    "    return utm.to_latlon(x[0], x[1], 32, 'T')\n",
    "# then we apply this function to the UTM coordinates in the file\n",
    "#df['LatLon'] = df[['Easting','Northing']].apply(uf, axis=1)\n",
    "df[['Lat', 'Lon']] = pd.DataFrame(df[['(UTM32)','(UTM32).1']].apply(uf, axis=1).tolist(), index=df.index)\n",
    "\n",
    "# removing the unwanted columns\n",
    "df = df.drop(columns = ['Unnamed: 0','Spor','(UTM32)','(UTM32).1'])\n",
    "\n",
    "# converting hours data columns into rows\n",
    "df = df.melt(id_vars=[\"Vej-Id\",\"Vejnavn\",\"Dato\",\"Lat\",\"Lon\"],\n",
    "        value_vars=['00','01','02','03','04','05','06','07','08','09','10','11','12',\n",
    "                    '13', '14','15', '16','17', '18','19','20','21','22','23'],\n",
    "        var_name=\"Hour\", \n",
    "        value_name=\"Vehicles\")\n",
    "\n",
    "### moving rows data for Vehicles Entering the City, Leaving the City and Net Traffic Flows into columns\n",
    "# Selecting only Entering Vehicles (and creating a unique index)\n",
    "df_ent = df[df['Vej-Id'] == '-']\n",
    "df_ent['index'] = df_ent['Vejnavn'] + df_ent['Dato'].dt.strftime('%Y-%m-%d') + df_ent['Hour']\n",
    "df_ent = df_ent.set_index('index')\n",
    "# Selecting only Leaving Vehicles (and creating a unique index)\n",
    "df_ex = df[df['Vej-Id'] == '+']\n",
    "df_ex['index'] = df_ex['Vejnavn'] + df_ex['Dato'].dt.strftime('%Y-%m-%d') + df_ex['Hour']\n",
    "df_ex = df_ex.set_index('index')\n",
    "# Selecting only Total Vehicles on the roads (and creating a unique index)\n",
    "df = df[df['Vej-Id'] == 'T']\n",
    "df['index'] = df['Vejnavn'] + df['Dato'].dt.strftime('%Y-%m-%d') + df['Hour']\n",
    "df = df.set_index('index')\n",
    "# adding columns for Vehicles Entering the City, Leaving and Net Traffic Flows\n",
    "df['Entering Vehicles'] = df_ent['Vehicles']\n",
    "df['Leaving Vehicles'] = df_ex['Vehicles']\n",
    "df['Net Traffic Flow'] = df['Entering Vehicles'] - df['Leaving Vehicles']\n",
    "# renaming the Total Vehicles column\n",
    "df = df.rename(columns={\"Vehicles\": \"Total Vehicles\"})\n",
    "\n",
    "# randomizing Latitude and longitude points\n",
    "mu, sigma1 = 0, 0.0015\n",
    "mu, sigma2 = 0, 0.003\n",
    "noise1 = np.random.normal(mu, sigma1, [len(df),1])\n",
    "noise2 = np.random.normal(mu, sigma2, [len(df),1]) \n",
    "df[['Lat_rand']] = df[['Lat']] + noise1\n",
    "df[['Lon_rand']] = df[['Lon']] + noise2\n",
    "\n",
    "# Add Day of the Week, Day, ,Week, Month, Year,\n",
    "df[\"DayName\"] = df['Dato'].apply(lambda x: x.day_name())\n",
    "df[\"WeekDay\"] = df['Dato'].dt.weekday\n",
    "df[\"DayOfMonth\"] = df['Dato'].dt.day\n",
    "df[\"Week\"] = df['Dato'].dt.week\n",
    "df[\"Month\"] = df['Dato'].dt.month\n",
    "df[\"Year\"] = df['Dato'].dt.year\n",
    "\n",
    "# removing the 'Vej-Id' columns to avoid confusion (now all traffic data are in the columns for \"-\" , \"+\" and \"T\")\n",
    "df = df.drop(columns = ['Vej-Id'])\n",
    "\n",
    "#df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96f4ea8",
   "metadata": {},
   "source": [
    "## Key points/plots from our exploratory data analysis\n",
    "\n",
    "### Total traffic distribution by Road\n",
    "\n",
    "\n",
    "Below is the count of (total) vehicles per each Road in descending order recorded in the whole period.\n",
    "From both the list and the plot below you can see how there are significant differences in traffic volumes among the roads available in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6f4dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "totcount = df.groupby('Vejnavn')['Total Vehicles'].sum().sort_values(ascending=False)\n",
    "pd.DataFrame(totcount.values, index = list(totcount.index), columns =['Total Vehicles']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8498c35c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(totcount.values, index = list(totcount.index), columns =['Vehicles']).plot.bar();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc53467",
   "metadata": {},
   "source": [
    "## Traffic distribution over time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1004d939",
   "metadata": {},
   "source": [
    "\n",
    "### Monthly distribution of total vehicles (All roads)\n",
    "\n",
    "The main pattern observable is the drop in traffic in July and December: this is probably due to the Danish holiday season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af12177d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('Month')['Total Vehicles'].sum().plot.bar();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2099950f",
   "metadata": {},
   "source": [
    "### Montly distribution per Road (Total vehicles)\n",
    "\n",
    "* below we plotted the montlhy distribution of total vehicles per road. The following roads show some monthly drops that could be due to data quality issues (missing data?):\n",
    "    - Englandsvej\n",
    "    - Hareskovvej\n",
    "    - Roskildevej\n",
    "    - Vejlands Allé\n",
    "    - Åboulevard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e577667a",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = df.groupby([\"Month\", \"Vejnavn\"]).sum()[\"Total Vehicles\"].unstack()\n",
    "#m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6e05f9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "m.plot(kind='bar', subplots=True, figsize=(15,60), layout=(9,4));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b03551",
   "metadata": {},
   "source": [
    "\n",
    "### Weekly distribution of total vehicles (All roads)\n",
    "\n",
    "#### The main pattern observable is the total traffic drop in the w-e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aad1688",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.groupby('WeekDay')['Total Vehicles'].sum().plot.bar();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16a8ec0",
   "metadata": {},
   "source": [
    "### Weekly distribution per Road (Total vehicles)\n",
    "#### The plots below show 2 main exceptions to the usual case of w-e drop in traffic:\n",
    "* **Kalvebod Brygge** where the drop happens on Mondays and tuesdays. Althought this could be due to some quality issue about the data (we need to check if this is still the case when we include all years (now we are only working with 2008 data)\n",
    "* **Jagtvej** shows a much lower drop in the w-e compared to other roads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706e59d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = df.groupby([\"WeekDay\", \"Vejnavn\"]).sum()[\"Total Vehicles\"].unstack()\n",
    "#w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce7e570",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "w.plot(kind='bar', subplots=True, figsize=(15,60), layout=(9,4));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088ba12f",
   "metadata": {},
   "source": [
    "\n",
    "### Day of Month distribution of total vehicles (All roads)\n",
    "\n",
    "The pattern that can is easily observed is that the 31st day of the month shows around half the volumes traffic compared to the other days of the month. This is of course due to the fact that there are ony 7 months out of 12 that contain 31 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb37368",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.groupby('DayOfMonth')['Total Vehicles'].sum().plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bde6a66",
   "metadata": {},
   "source": [
    "### Day of the month distribution per Road (Total vehicles)\n",
    "\n",
    "* when plotting the day-of-the-month distribution for each road we notice that a few roads show strange patterns that could signal some data quality issues:\n",
    "    - Hareskovvej\n",
    "    - Roskildevej"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239d5968",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "d = df.groupby([\"DayOfMonth\", \"Vejnavn\"]).sum()[\"Total Vehicles\"].unstack()\n",
    "#d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262d0336",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "d.plot(kind='bar', subplots=True, figsize=(15,60), layout=(9,4));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b495df76",
   "metadata": {},
   "source": [
    "### Hourly distribution per Road (Total vehicles)\n",
    "**Nearly all roads share the same pattern in hourly traffic flows:**\n",
    "- Midnight to 5am: very low traffic \n",
    "- 6-8am: people go to work and vehicles volumes increase rapidly for 3 hours. Then a little slow down for a couple of hours\n",
    "- 11 to 15-16: traffic volumes start growing again until they peak when people start going bach home from work. \n",
    "- 17-18: vehicles numbers drop consistently. Dinner time in Denmark\n",
    "- 19-23: the traffic flows slowly reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd69c234",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = df.groupby([\"Hour\", \"Vejnavn\"]).sum()[\"Total Vehicles\"].unstack()\n",
    "#h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9808dd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "h.plot(kind='bar', subplots=True, figsize=(15,60), layout=(9,4));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea91a77",
   "metadata": {},
   "source": [
    "### Hourly distribution per Road (Net Entry-Exit flows)\n",
    "**NOTE: the Exit-Entry analysis is based on one big assumptions:** roads numbering follows an ascending order that starts at zero from the City center and then increases with the distance from the City center\n",
    "\n",
    "**Most roads share the same pattern shapes, but not all:**\n",
    "- Midnight to 4am: most roads show cars exiting the city\n",
    "- 5-9am: vehicles entering the city are the majority and inward volumes constantly increase until they peak around 9-10\n",
    "- 11 to 15-16: inward traffic volumes start dwindling until the outwarding vehicles start taking over from 15\n",
    "- 15-18: majority of vehicles are the one exiting the city\n",
    "- 19-23: no clear pattern: some roads show the majority of vehicles enterring the city again, while others show the majority of cars exiting the city, depending on the timeframe\n",
    "- some roads, like Roskildevej and Torvegade show opposite patterns than the one described above. The reason is probably because these roads lead to specific locations that attract a high number of workers, respectively **Roskilde and Amager**\n",
    "- some other roads instead show systematically net inward or systematic net outward flow of vehicles during the whole day. These are, respectively:\n",
    "  - Englandsvej (inward flow)\n",
    "  - Islands Brygge (inward flow)\n",
    "  - Skt. Kjelds Gade\n",
    "  - Sølvgade\n",
    "  - Molbechsvej (ouward flow)\n",
    "  - Vejlands Allé (ouward flow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263d83e2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hd = df.groupby([\"Hour\", \"Vejnavn\"]).sum()[\"Net Traffic Flow\"].unstack()\n",
    "#hd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc422391",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hd.plot(kind='bar', subplots=True, figsize=(15,60), layout=(9,4));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functional-parallel",
   "metadata": {},
   "source": [
    "### Yearly distribution of total vehicles (All roads)\n",
    "\n",
    "* when plotting the Yearly distribution of the total traffic across all roads between 2005 and 2014 we notice a strange pattern: the traffic increases from 2005 to 2008 and it decreases steadily from 2008 to 2014.\n",
    "* before we draw any conlusion about this we better make some additional checks about data quality\n",
    "* we will see that plotting the yarly distribution of traffic for each road will show some severe issues with the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olympic-peripheral",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('Year')['Total Vehicles'].sum().plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387e0a87",
   "metadata": {},
   "source": [
    "## Yearly distribution per Road\n",
    "\n",
    "as you can see from the plots below, the Yearly distribution of total traffic data raises some concerns about the quality of the data:\n",
    "\n",
    "* some roads are missing entire years of data:\n",
    "    - Englandsvej\n",
    "    - Hareskovvej\n",
    "    - Jagtvej\n",
    "    - Kalkbrænderihavnsgade\n",
    "    - Molbechsvej\n",
    "    - Mozartsvej\n",
    "    - Roskildevej\n",
    "    - Skt. Kjelds Gade\n",
    "    - Vejlands Allé\n",
    "    - Åboulevard\n",
    "    - Ørestads Boulevard\n",
    "* most other roads have suspicious drops in the total traffic numbers in some years. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afc8090",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.groupby([\"Year\", \"Vejnavn\"]).sum()[\"Total Vehicles\"].unstack()\n",
    "#y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027c5979",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y.plot(kind='bar', subplots=True, figsize=(15,60), layout=(9,4));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patient-sussex",
   "metadata": {},
   "source": [
    "# Using visualizations to check for missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latest-underground",
   "metadata": {},
   "source": [
    "### Counting the number of data rows (days of data) for each Road\n",
    "\n",
    "the previous plots have already suggested that there were both actual and potential issues about Traffic data quality. So we  decided to run some sense check about the quality of the data\n",
    "\n",
    "* as a simple completeness test we checked how many Roads have around 365 rows (days) of data for each year. The plots below confirmed our suspects:\n",
    "    - no Road has the full list of 365 rows of data for all the available years\n",
    "    - only a few Roads have 365 rows of data in the years 2011-2014, which is the time window for which the Weather data are available:\n",
    "        - Ellebjergvej\n",
    "        - Gadelandet\n",
    "        - Islands Brygge\n",
    "        - Kalvebod Brygge\n",
    "        - Torvegade\n",
    "* we will use this finding when we will build our ML test later in the NB to make sure that that the data issues don't affect our Classifier. **Basically we will select only the Roads above for our ML exercise**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fuzzy-smith",
   "metadata": {},
   "outputs": [],
   "source": [
    "yc = df.groupby([\"Year\", \"Vejnavn\"]).count()[\"Total Vehicles\"].unstack()/24\n",
    "yc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "administrative-proportion",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "yc.plot(kind='bar', subplots=True, figsize=(15,60), layout=(9,4));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "official-representative",
   "metadata": {},
   "source": [
    "# Visualization of Traffic volumes for each CPH district over time\n",
    "\n",
    "* first we find CPH districts through a GeoJson file\n",
    "* then we map CPH roads to the relevant districts \n",
    "* then we add the distric information to the main DataFrame\n",
    "* then we group the district data by timeframe: weekly/hourly seem like interesting timeframes to investigate\n",
    "* finally we represent the data on the interactive Choropleth Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrapped-rotation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we upload CPH districts polygons from GeoJson file\n",
    "import urllib.request, json \n",
    "\n",
    "with urllib.request.urlopen(\"https://raw.githubusercontent.com/LarsBryld/socialdata/main/copenhagen_districts.geojson\") as url:\n",
    "    cph_districts = json.loads(url.read().decode())\n",
    "\n",
    "# we extract the districts names from the GeoJson file\n",
    "districts = []\n",
    "for i in range(len(cph_districts[\"features\"])):\n",
    "    districts.append(cph_districts[\"features\"][i]['properties']['name'])\n",
    "    \n",
    "#districts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acceptable-amino",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# then we create a unique list of CPH roads, with corresponding Longitude and Latitute\n",
    "dfu = pd.concat((pd.DataFrame(df['Vejnavn'].unique(),columns=['Vejnavn']),\n",
    "           pd.DataFrame(df['Lon'].unique(),columns=['Lon']),\n",
    "           pd.DataFrame(df['Lat'].unique(),columns=['Lat'])), axis=1)\n",
    "#dfu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "architectural-formula",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally we find in what district each road falls, using the code from this example:\n",
    "# https://stackoverflow.com/questions/57727739/how-to-determine-if-a-point-is-inside-a-polygon-using-geojson-and-shapely\n",
    "\n",
    "from shapely.geometry import shape, GeometryCollection, Point\n",
    "\n",
    "dist = []\n",
    "\n",
    "for i in range(len(dfu)):\n",
    "    for j in range(len(districts)):\n",
    "        if shape(cph_districts[\"features\"][j]['geometry']).contains(Point(dfu['Lon'][i],dfu['Lat'][i])):\n",
    "            dist.append(cph_districts[\"features\"][j]['properties']['name'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modern-sacrifice",
   "metadata": {},
   "outputs": [],
   "source": [
    "# then we add the district name to our list of unique roads\n",
    "dfu = pd.concat((dfu, pd.DataFrame(dist,columns=['District'])), axis=1)\n",
    "dfu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocational-rating",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the District info to the original DF\n",
    "df = pd.merge(df, dfu[['Vejnavn','District']], on=\"Vejnavn\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solved-highlight",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# grouping traffic data by Year and District\n",
    "dfc = df.groupby([\"Year\", \"District\"]).mean()[\"Total Vehicles\"].unstack()\n",
    "dfc = dfc.reset_index()\n",
    "dfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civil-bracelet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the Plotly Choropleth to work we nee to put all columns into rows \n",
    "\n",
    "dfc = dfc.melt(id_vars=['Year'],\n",
    "        value_vars=dfc.columns.values[1:],\n",
    "        var_name=\"District\", \n",
    "        value_name=\"Total Vehicles\")\n",
    "dfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invisible-poverty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating an interactive map for CPH traffic\n",
    "import plotly.express as px\n",
    "\n",
    "max_value = dfc['Total Vehicles'].max()\n",
    "fig = px.choropleth(dfc, locations='District',\n",
    "                    geojson=cph_districts, featureidkey=\"properties.name\",\n",
    "                           color='Total Vehicles',\n",
    "                           color_continuous_scale=\"Viridis\",\n",
    "                           range_color=(0, max_value),\n",
    "                           projection=\"mercator\",\n",
    "                    animation_frame=\"Year\", animation_group=\"District\"\n",
    "                          )\n",
    "\n",
    "fig.update_geos(fitbounds=\"locations\", visible=False)\n",
    "fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forced-sheet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouping traffic data by Hour and District\n",
    "dfh = df.groupby([\"Hour\", \"District\"]).mean()[\"Total Vehicles\"].unstack()\n",
    "dfh = dfh.reset_index()\n",
    "dfh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "better-equivalent",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the Plotly Choropleth to work we nee to put all columns into rows \n",
    "\n",
    "dfh = dfh.melt(id_vars=['Hour'],\n",
    "        value_vars=dfh.columns.values[1:],\n",
    "        var_name=\"District\", \n",
    "        value_name=\"Total Vehicles\")\n",
    "dfh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blessed-spiritual",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# creating an interactive map for CPH traffic\n",
    "import plotly.express as px\n",
    "\n",
    "max_value = dfh['Total Vehicles'].max()\n",
    "min_value = dfh['Total Vehicles'].min()\n",
    "fig = px.choropleth(dfh, locations='District',\n",
    "                    geojson=cph_districts, featureidkey=\"properties.name\",\n",
    "                           color='Total Vehicles',\n",
    "                           color_continuous_scale=\"Viridis\",\n",
    "                           range_color=(min_value, max_value),\n",
    "                           projection=\"mercator\",\n",
    "                    animation_frame=\"Hour\", animation_group=\"District\"\n",
    "                          )\n",
    "\n",
    "fig.update_geos(fitbounds=\"locations\", visible=False)\n",
    "fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0b7be2",
   "metadata": {},
   "source": [
    "# Data Analysis\n",
    "### Describe your data analysis and explain what you've learned about the dataset.\n",
    "Our data visualizations have shown us that:\n",
    "- there are huge differences in traffic across different roads in Copenhagen. This is of course very intuitive because different roads serve different purposes: the big traffic arteries have been build with the aim of carrying most of the daily traffic inside and outside of the city, while smaller ones are only used for local traffic. A good example of 2 roads like these are, for example, Ellebjergvej and Mozartsvej. These 2 roads are very close to each other but they serve completely different purposes: \n",
    "    - Ellebjergvej allows people to get in and out of the city and is one of the busiest roads in our dataset, while \n",
    "    - Mozartsvej only serves the local traffic and is the second less busy road in the dataset.\n",
    "- Monthly traffic is pretty stable across all months, with 2 biggest exceptions during Danish holidays, when the traffic slows down compared to the other months: July and December\n",
    "- Weekly traffic in Copenhagen decreases in the weekend, as expected, for all roads\n",
    "- Hourly (total) traffic shows a pretty consistent pattern across all roads: very low traffic during the night; increase in the early hours of the day (6-8am) when people go to work. It keeps growing until it reaches a peak around 15-16 when people start going back home from work. Beween 16-18 the traffic quickly decreases, and after dinner time (7pm) it slows down a lot.\n",
    "- Hourly net flows also show a pretty interesting and consistent pattern: \n",
    "    - for most roads the net traffic flow outside of the city during the night; \n",
    "    - the majority of vehicles starts flowing to the city from 4am, and the process continues until it peaks around 10-11: people leaving outside of the city go to their workplaces or to businesses that are located inside the city; \n",
    "    - after 11am the net inflow of cars starts going down and \n",
    "    - around 15 the majority of vehicles on the roads are the ones leaving the city: the end of the working day starts approaching.\n",
    "    - there is only one big exception to the above net traffic flow: Roskildevej shows a pattern opposite to all other roads. This probably suggests that the number of Copehageners working in Roskilde is higher that the number of people living in Roskilde and working in CPH \n",
    "    - a handful of other roads **(......)** don't show the hourly net flow of traffic that we have just destribed, and instead these show one-sided traffic flow across the whole day. This is probably due to the fact that our assumption about identifying the traffic direction with the Vej-Id doesn't hold for these roads.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d951a49",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "\n",
    "## ML Exercise 1: classifying traffic data by Roads by using hourly Net Traffic Flows\n",
    "\n",
    "The visualization of Hourly Net Flows (vehicles leaving - vehicles entering the city) above shows that, for example, Roskildevej has a very different pattern from other roads: cars on Roskildevej are leaving the city in the morning and are coming back at night. This is probably due to the fact that the number of Copenhageners working in Roskilde is higher than the number of Roskilde residents working in Copenhagen.\n",
    "\n",
    "Based on this visual information we have build a classifier that can identify traffic flows on Roskildevej from roads that have a completely different hourly traffic flow (we have chosen Ellebjergvej for our example).\n",
    "\n",
    "Our classifier uses Random Forests and yields around 80% accurate predictions for both the training and test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b899f146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test without Weather data\n",
    "\n",
    "### data preprocessing\n",
    "\n",
    "# selecting our sample: focussing on data for 2011, 2012, 2013 and 2 roads only\n",
    "dfml = df[(df['Year'].isin([2011,2012,2013]))\n",
    "         & (df['Vejnavn'].isin(['Ellebjergvej','Roskildevej']))]  \n",
    "\n",
    "# keeping only: Hour-of-the-day, Day-of-the-week, Month-of-the-year, and PD-District\n",
    "dfml = dfml[['Vejnavn','Net Traffic Flow', 'Hour']]\n",
    "#dfml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c9798e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding data with LabelEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#creating labelencoder\n",
    "le = LabelEncoder()\n",
    "# encoding string labels (Category is the target variable)\n",
    "labels = le.fit_transform(dfml['Vejnavn'])\n",
    "\n",
    "# encoding string features (not necessary)\n",
    "#NetTraffic =le.fit_transform(dfml['Hour'])\n",
    "\n",
    "features=dfml[['Net Traffic Flow', 'Hour']]\n",
    "\n",
    "#Split Train/test datasets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.33, random_state=42)\n",
    "\n",
    "# Random Forest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=10, max_depth=None, min_samples_split=2, random_state=0)\n",
    "clf = clf.fit(X_train,  y_train)\n",
    "\n",
    "# measuring the classification performance of the RF classifier through cross_val_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "np.mean(cross_val_score(clf, X_train,  y_train, cv=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e74b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measuring RF prediction performance (in the Test sample)\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = clf.predict(X_test) # 0:Overcast, 2:Mild\n",
    "print(classification_report(y_test, y_pred))#, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3c3cbe",
   "metadata": {},
   "source": [
    "## ML Exercise 2: predict traffic volumes with weather data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1359a2d",
   "metadata": {},
   "source": [
    "### Importing CPH weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2535c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfw = pd.read_csv(\"https://raw.githubusercontent.com/LarsBryld/socialdata/main/cph_weather.csv\",\n",
    "                 parse_dates = ['DateTime'])\n",
    "dfw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reflected-blake",
   "metadata": {},
   "source": [
    "### Merging Traffic and Weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a7f364",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfm = pd.merge(df, dfw, left_on='Dato',right_on='DateTime')\n",
    "dfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a62631",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "roadsforML = ['Ellebjergvej',\n",
    "              'Gadelandet',\n",
    "              'Islands Brygge',\n",
    "              'Kalvebod Brygge',\n",
    "              'Torvegade'\n",
    "]\n",
    "yearsforML = [2011, 2012, 2013]\n",
    "\n",
    "dfml = dfm[(dfm['Vejnavn'].isin(roadsforML)) & \n",
    "           (dfm['Year'].isin(yearsforML)) &\n",
    "           (dfm['WeekDay']<=4) &\n",
    "           (~dfm['Month'].isin([1,7,12]))]\n",
    "\n",
    "dfml = dfml.groupby(['Dato']).mean()\n",
    "\n",
    "dfml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fantastic-tours",
   "metadata": {},
   "source": [
    "### let's have a look at the data before we check our ML engine: \n",
    "* the subset of data that we selected seem to be stable across the 3 years that we are investigating, rather than decreasing wehn we looked at the overall dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collect-fireplace",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfml.groupby('Year')['Total Vehicles'].sum().plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confidential-moderator",
   "metadata": {},
   "source": [
    "### when investigating the distribution of traffic days between: low, medium and high traffic days, we notice some class imbalance in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f074ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfml['Total Vehicles'].plot.hist(bins=3, alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dutch-leave",
   "metadata": {},
   "source": [
    "### the Random Forrest Classifier on the current data shows a modest predicting power of 59%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac8407d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#creating labes by splitting floating data into bins\n",
    "labels = pd.cut(dfml['Total Vehicles'], bins=3, retbins=True, labels=False)[0]\n",
    "\n",
    "# encoding string features is not necessary\n",
    "features = dfml[[\"LowTemp\",\n",
    "               \"HighTem\",\n",
    "               \"MidTemp\",\n",
    "               \"AirPressure\",\n",
    "               \"Rain\",\n",
    "               \"LowWind\",\n",
    "               \"MidWind\",\n",
    "               \"HighWind\",\n",
    "               \"Sunshine\"]]\n",
    "\n",
    "#Split Train/test datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.33, random_state=42)\n",
    "\n",
    "# Random Forest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=10, max_depth=None, min_samples_split=2, random_state=0)\n",
    "clf = clf.fit(X_train,  y_train)\n",
    "\n",
    "\n",
    "\n",
    "# measuring the classification performance of the RF classifier through cross_val_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "np.mean(cross_val_score(clf, X_train,  y_train, cv=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80f384a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measuring RF prediction performance (in the Test sample)\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = clf.predict(X_test) # 0:Overcast, 2:Mild\n",
    "print(classification_report(y_test, y_pred))#, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "banned-ordering",
   "metadata": {},
   "source": [
    "### one way to try improve the classifier is to resample the data to eliminate the class imbalance in the data \n",
    "\n",
    "### so we need to increase the sample size of classes 0 and 1 (low and medium traffic respectively) and then see if the classifier improves\n",
    "\n",
    "### how do we do it? first we add the labels to our dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6222222c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfml['Labels'] = labels\n",
    "#dfml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa13332",
   "metadata": {},
   "source": [
    "### then we split the dataframe by:\n",
    "- Train and Test samples\n",
    "- then we re-sample each category to have an equally weighted Train sample\n",
    "- we train the sample on the balanced dataset and finally we check the model predictive power on the origial Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "different-spectacular",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split Train/test datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(features, labels, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innocent-reserve",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the training dataset now contains both features and labels\n",
    "TrainSet = pd.concat((y_train1,X_train1), axis=1)\n",
    "TrainSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb479c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we fix the class imbalance and the by re-sampling and inceasing the trainign dataset to 1000 elements for each class\n",
    "# setting Max Class Size equal to 1.000\n",
    "mcs = 1000\n",
    "\n",
    "# extracting random samples of equally sized classes\n",
    "dfs = pd.DataFrame()\n",
    "dfs = dfs.append(TrainSet[TrainSet['Total Vehicles'] == 0].sample(n=mcs, random_state=1, replace=True))\n",
    "dfs = dfs.append(TrainSet[TrainSet['Total Vehicles'] == 1].sample(n=mcs, random_state=1, replace=True))\n",
    "dfs = dfs.append(TrainSet[TrainSet['Total Vehicles'] == 2].sample(n=mcs, random_state=1, replace=True)) \n",
    "\n",
    "dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0431a03f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfs['Total Vehicles'].plot.hist(bins=5, alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hybrid-cedar",
   "metadata": {},
   "source": [
    "### The data resampling drastically increases the in-sample predictive power of the classifier to nearly 100%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe9a4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#taking the labels from the df\n",
    "labels1 = dfs['Total Vehicles']\n",
    "\n",
    "# tafing weather features from the df\n",
    "features1 = dfs[[\"LowTemp\",\n",
    "               \"HighTem\",\n",
    "               \"MidTemp\",\n",
    "               \"AirPressure\",\n",
    "               \"Rain\",\n",
    "               \"LowWind\",\n",
    "               \"MidWind\",\n",
    "               \"HighWind\",\n",
    "               \"Sunshine\"]]\n",
    "\n",
    "#Split Train/test datasets\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#X_train1, X_test1, y_train1, y_test1 = train_test_split(features, labels, test_size=0.33, random_state=42)\n",
    "\n",
    "# Random Forest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf1 = RandomForestClassifier(n_estimators=10, max_depth=None, min_samples_split=2, random_state=0)\n",
    "#clf1 = clf1.fit(X_train1, y_train1)\n",
    "clf1 = clf1.fit(features1, labels1)\n",
    "\n",
    "# measuring the classification performance of the RF classifier through cross_val_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "np.mean(cross_val_score(clf1, features1, labels1, cv=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adult-brisbane",
   "metadata": {},
   "source": [
    "### the predictive power on the Test sample instead has not increased much: only from 59% to 61%\n",
    "* this result makes us conclude that the weather only has a modest predictive power in predicting the traffic flow\n",
    "* this is probably due to 2 issues:\n",
    "    - there is not enough granularity in the data: \"Vehicles\" can include both cars and bicycles for example, while the choic of which vehicle is used could depend on the weather\n",
    "    - the decision of driving/cycling (or driving any other vehicle) is due to reasons that are independent of the weather (family commitments; job needs; etc.)\n",
    "    - considering all the above we still believe that the weather has some predicting power on traffic volumes and it could be improved with higher data granularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instructional-tobacco",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(cross_val_score(clf1, X_test1, y_test1, cv=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff14339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measuring RF prediction performance (in the Test sample)\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred1 = clf1.predict(X_test1) # 0:Overcast, 2:Mild\n",
    "print(classification_report(y_test1, y_pred1))#, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd08ea9",
   "metadata": {},
   "source": [
    "### Let's find which weather data are driving the classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f98c081",
   "metadata": {},
   "outputs": [],
   "source": [
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "\n",
    "perm = PermutationImportance(clf, random_state=1).fit(X_test, y_test)\n",
    "eli5.show_weights(perm, feature_names = X_test.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a021e991",
   "metadata": {},
   "outputs": [],
   "source": [
    "perm = PermutationImportance(clf1, random_state=1).fit(X_test1, y_test1)\n",
    "eli5.show_weights(perm, feature_names = X_test1.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revolutionary-destiny",
   "metadata": {},
   "source": [
    "# Genre. Which genre of data story did you use?\n",
    "* Which tools did you use from each of the 3 categories of Visual Narrative (Figure 7 in Segal and Heer). Why?\n",
    "* Which tools did you use from each of the 3 categories of Narrative Structure (Figure 7 in Segal and Heer). Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capable-guatemala",
   "metadata": {},
   "source": [
    "# Visualizations.\n",
    "* Explain the visualizations you've chosen.\n",
    "* Why are they right for the story you want to tell?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funded-legend",
   "metadata": {},
   "source": [
    "# Discussion. Think critically about your creation\n",
    "* What went well?\n",
    "    - We managed to get all the data we were looking for from Open data facilities: Traffic, Weather, Maps Polygons\n",
    "    - The cleaning/processing/merging of all data visualization purposes\n",
    "    - We were able to extract useful informations from the our visualizations\n",
    "    - Ther Machine Learning exercises gave us some hints about the predictability of traffic flows\n",
    "    - The website representation was far from obvious to address, but we managed to do it, although it required a lot of effort\n",
    "    - We believe that our story representation allows the end user understand the main features/information that the data contain\n",
    "* What is still missing? What could be improved?, Why?\n",
    "    - with more granularity in the data (split between different vehicles for example) we might have had a better Classifier/Predictor of Traffic volumes based on Weather conditions\n",
    "    - to the same aim we could/should have normalised the weather data by month, although the impact of this is far from obvious\n",
    "    - ....."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authorized-joint",
   "metadata": {},
   "source": [
    "# Contributions. Who did what?\n",
    "You should write (just briefly) which group member was the main responsible for which elements of the assignment. (I want you guys to understand every part of the assignment, but usually there is someone who took lead role on certain portions of the work. That's what you should explain)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protective-poland",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "celtic-auction",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['WeekDay','Vejnavn']).sum()#[\"Net Traffic Flow\"]\n",
    "#df.groupby([\"Year\", \"Vejnavn\"]).count()[\"Total Vehicles\"].unstack()/24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "potential-active",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(w, x=\"WeekDay\", y=\"Net Traffic Flow\", facet_col='Vejnavn', facet_col_wrap=5)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "special-surveillance",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_canada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "catholic-essence",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "data_canada = px.data.gapminder().query(\"country == 'Canada'\")\n",
    "fig = px.bar(data_canada, x='year', y='pop')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesser-jason",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_italia = px.data.gapminder().query(\"country == 'Italy'\")\n",
    "data_italia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viral-contemporary",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat((data_canada,data_italia))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordinary-cisco",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "fig = px.bar(data, x='year', y='pop')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1db0fd9",
   "metadata": {},
   "source": [
    "# Additional space data visualizations using Folium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995ee389",
   "metadata": {},
   "source": [
    "# Radhusplads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dfa27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "\n",
    "map_hooray = folium.Map([55.6761, 12.5683], tiles = \"Stamen Toner\", zoom_start=12.5)\n",
    "\n",
    "folium.Marker([55.6761, 12.5683], \n",
    "              popup='RadHus Plads', \n",
    "              icon=folium.Icon(color='blue')\n",
    "             ).add_to(map_hooray)\n",
    "\n",
    "map_hooray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbaa095a",
   "metadata": {},
   "source": [
    "# Visualizing some traffic data (randomized locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c51edfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df[(df['DayOfMonth'].isin([1,2,3,4,5,2,3,4,5,6,7,8,9,10,\n",
    "                                11,12,13,14,15,16,17,18,19,20,\n",
    "                                21,22,23,24,25,26,27,28,29,30,31]))\n",
    "         & (df['Hour'].isin(['07','08']))\n",
    "         & (df['Month'].isin([6, 7]))\n",
    "         & (df['Year'] == 2012)]\n",
    "\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0d0efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "map2 = folium.Map([55.6761, 12.5683], tiles = \"Stamen Toner\", zoom_start=12)\n",
    "\n",
    "folium.Marker([55.6761, 12.5683], \n",
    "              popup='City Hall', \n",
    "              icon=folium.Icon(color='blue')\n",
    "             ).add_to(map2)\n",
    "\n",
    "for i in range(len(df1)):\n",
    "    folium.Circle(location=[df1.iloc[i]['Lat_rand'], df1.iloc[i]['Lon_rand']],\n",
    "                  popup=df1.iloc[i]['Month'],\n",
    "                  radius=4, #data.iloc[i]['value']*10000,\n",
    "                  color='crimson',\n",
    "                  fill=True,\n",
    "                  fill_color='crimson'\n",
    "                 ).add_to(map2)\n",
    "\n",
    "map2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01156396",
   "metadata": {},
   "source": [
    "# Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b988a9e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from folium.plugins import HeatMap\n",
    "\n",
    "map_hooray = folium.Map([55.6761, 12.5683], tiles = \"Stamen Toner\", zoom_start=12)\n",
    "\n",
    "# Filter the DF for rows, then columns, then remove NaNs\n",
    "heat_df = df1[['Lat', 'Lon']]\n",
    "#heat_df = heat_df.dropna(axis=0, subset=['Y','X'])\n",
    "\n",
    "\n",
    "\n",
    "# List comprehension to make out list of lists\n",
    "heat_data = [[row['Lat'],row['Lon']] for index, row in heat_df.iterrows()]\n",
    "\n",
    "# Plot it on the map\n",
    "HeatMap(heat_data).add_to(map_hooray)\n",
    "\n",
    "# Display the map\n",
    "map_hooray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9df7c9a",
   "metadata": {},
   "source": [
    "# HeatMapWithTime  (Weekdays - NON random locations) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938f61e0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from folium import plugins\n",
    "\n",
    "map_hooray = folium.Map([55.6761, 12.5683], tiles = \"Stamen Toner\", zoom_start=12)\n",
    "\n",
    "# Filter the DF for rows, then columns, then remove NaNs\n",
    "heat_df = df1[['Lat', 'Lon','Lat_rand', 'Lon_rand']]\n",
    "#heat_df = heat_df.dropna(axis=0, subset=['Y','X'])\n",
    "\n",
    "# List comprehension to make out list of lists\n",
    "heat_data = [[row['Lat'],row['Lon']] for index, row in heat_df.iterrows()]\n",
    "\n",
    "# Create weight column, using date\n",
    "heat_df['Weight'] = df1['WeekDay']\n",
    "heat_df['Weight'] = heat_df['Weight'].astype(float)\n",
    "heat_df = heat_df.dropna(axis=0, subset=['Lat','Lon', 'Weight'])\n",
    "\n",
    "# List comprehension to make out list of lists\n",
    "heat_data = [[[row['Lat'],row['Lon']] for index, row in heat_df[heat_df['Weight'] == i].iterrows()] for i in range(0,7)]\n",
    "\n",
    "# Plot it on the map\n",
    "hm = plugins.HeatMapWithTime(heat_data,auto_play=True,max_opacity=0.8)\n",
    "hm.add_to(map_hooray)\n",
    "# Display the map\n",
    "map_hooray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6c9abd",
   "metadata": {},
   "source": [
    "# HeatMapWithTime  (Weekdays - randomized locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1efea6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from folium import plugins\n",
    "\n",
    "map_hooray = folium.Map([55.6761, 12.5683], tiles = \"Stamen Toner\", zoom_start=12)\n",
    "\n",
    "heat_rn = pd.concat([heat_df['Lat_rand'], heat_df['Lon_rand'], heat_df['Weight']], axis=1)\n",
    "\n",
    "# List comprehension to make out list of lists\n",
    "heat_data_rn = [[[row['Lat_rand'],row['Lon_rand']] for index, row in heat_rn[heat_rn['Weight'] == i].iterrows()] for i in range(0,7)]\n",
    "\n",
    "# Plot it on the map\n",
    "hm = plugins.HeatMapWithTime(heat_data_rn,auto_play=True,max_opacity=0.8)\n",
    "hm.add_to(map_hooray)\n",
    "# Display the map\n",
    "map_hooray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfabf78b",
   "metadata": {},
   "source": [
    "# HeatMapWithTime  (Hours - only 2 roads) - we could skip this one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cf2dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df[(df['DayOfMonth'].isin([1,2,3,4,5,2,3,4,5,6,7,8,9,10,\n",
    "                                11,12,13,14,15,16,17,18,19,20,\n",
    "                                21,22,23,24,25,26,27,28,29,30,31]))\n",
    "         & (df['Vejnavn'].isin(['Roskildevej','Ellebjergvej']))\n",
    "#         & (df['Hour'].isin(['07','08']))\n",
    "         & (df['Month'].isin([6, 7]))\n",
    "         & (df['Year'] == 2012)]\n",
    "#        & (df['Vej-Id'] == 'T')]\n",
    "\n",
    "#df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804c1810",
   "metadata": {},
   "outputs": [],
   "source": [
    "map3 = folium.Map([55.6761, 12.5683], tiles = \"Stamen Toner\", zoom_start=12)\n",
    "\n",
    "folium.Marker([55.6761, 12.5683], \n",
    "              popup='City Hall', \n",
    "              icon=folium.Icon(color='blue')\n",
    "             ).add_to(map3)\n",
    "\n",
    "for i in range(len(df3)):\n",
    "    folium.Circle(location=[df3.iloc[i]['Lat_rand'], df3.iloc[i]['Lon_rand']],\n",
    "                  popup=df3.iloc[i]['Month'],\n",
    "                  radius=4, #data.iloc[i]['value']*10000,\n",
    "                  color='crimson',\n",
    "                  fill=True,\n",
    "                  fill_color='crimson'\n",
    "                 ).add_to(map3)\n",
    "\n",
    "map3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9858ca04",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from folium import plugins\n",
    "\n",
    "map_hooray = folium.Map([55.6761, 12.5683], tiles = \"Stamen Toner\", zoom_start=12)\n",
    "\n",
    "# Filter the DF for rows, then columns, then remove NaNs\n",
    "heat_df = df3[['Lat_rand', 'Lon_rand']]\n",
    "#heat_df = heat_df.dropna(axis=0, subset=['Y','X'])\n",
    "\n",
    "# List comprehension to make out list of lists\n",
    "heat_data = [[row['Lat_rand'],row['Lon_rand']] for index, row in heat_df.iterrows()]\n",
    "\n",
    "# Create weight column, using date\n",
    "heat_df['Weight'] = df3['Hour']\n",
    "heat_df['Weight'] = heat_df['Weight'].astype(float)\n",
    "heat_df = heat_df.dropna(axis=0, subset=['Lat_rand','Lon_rand', 'Weight'])\n",
    "\n",
    "# List comprehension to make out list of lists\n",
    "heat_data = [[[row['Lat_rand'],row['Lon_rand']] for index, row in heat_df[heat_df['Weight'] == i].iterrows()] for i in range(0,24)]\n",
    "\n",
    "# Plot it on the map\n",
    "hm = plugins.HeatMapWithTime(heat_data,auto_play=True,max_opacity=0.8)\n",
    "hm.add_to(map_hooray)\n",
    "# Display the map\n",
    "map_hooray"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
